{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e147e6",
   "metadata": {},
   "source": [
    "# XGBoost Training — Starter Notebook (Aligned with `xgboost-training/`)\n",
    "This notebook is designed for the directory layout:\n",
    "```\n",
    "xgboost-training/\n",
    "├─ data/{raw,processed}\n",
    "├─ notebooks/\n",
    "└─ src/\n",
    "```\n",
    "It uses built-in scikit-learn datasets (no downloads), and is easy to extend to files under `data/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bea968",
   "metadata": {},
   "source": [
    "## Environment & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c978d7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed in your local machine, install deps in your active virtualenv:\n",
    "# !pip install xgboost scikit-learn matplotlib shap ipykernel\n",
    "\n",
    "import os, sys, json, math, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, RocCurveDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8960b186",
   "metadata": {},
   "source": [
    "## Project Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930a3e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Project root is assumed to be the parent of this notebook directory\n",
    "NB_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NB_DIR.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "\n",
    "for p in [DATA_DIR, RAW_DIR, PROC_DIR, SRC_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"RAW_DIR:\", RAW_DIR)\n",
    "print(\"PROC_DIR:\", PROC_DIR)\n",
    "print(\"SRC_DIR:\", SRC_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3c1b7",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Phase 1 — Core Concepts](#phase1)\n",
    "   - [Iris (Multiclass)](#iris)\n",
    "   - [Diabetes (Regression)](#diabetes)\n",
    "   - [Breast Cancer (Binary + ROC/AUC)](#cancer)\n",
    "2. [Phase 2 — Pipelines & Early Stopping](#phase2)\n",
    "   - [Synthetic Classification with Categorical + Numeric](#synthetic)\n",
    "   - [Regression with Early Stopping + Feature Importance](#early)\n",
    "3. [Extras — Simple Hyperparameter Search](#extras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc243ac3",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"phase1\"></a>\n",
    "# Phase 1 — Core Concepts\n",
    "Small, clean datasets to learn XGBoost mechanics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883d97a3",
   "metadata": {},
   "source": [
    "<a id=\"iris\"></a>\n",
    "## 1.1 Iris — Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3bbe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "clf = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective='multi:softmax',\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, pred, target_names=iris.target_names))\n",
    "\n",
    "xgb.plot_importance(clf, height=0.5)\n",
    "plt.title(\"Feature Importance — Iris\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce385858",
   "metadata": {},
   "source": [
    "## Visualizing XGBoost Tree Architecture on Iris\n",
    "\n",
    "Below we plot the **first few trees** from our Iris classifier to understand:\n",
    "- How they split features\n",
    "- How they output leaf values (log-odds contributions)\n",
    "- How multiple trees per boosting round are used for multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f745714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Show tree 0 (first boosting round, class 0)\n",
    "plot_tree(clf, num_trees=0)\n",
    "plt.title(\"Tree 0 — Round 1, Class 0\")\n",
    "plt.show()\n",
    "\n",
    "# Show tree 1 (first boosting round, class 1)\n",
    "plot_tree(clf, num_trees=1)\n",
    "plt.title(\"Tree 1 — Round 1, Class 1\")\n",
    "plt.show()\n",
    "\n",
    "# Show tree 2 (first boosting round, class 2)\n",
    "plot_tree(clf, num_trees=2)\n",
    "plt.title(\"Tree 2 — Round 1, Class 2\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: print raw text dump for first tree\n",
    "print(\"\\nRaw text dump for Tree 0:\")\n",
    "print(clf.get_booster().get_dump()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad60db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Take a few test samples\n",
    "X_sample = X_test[:5]\n",
    "\n",
    "# Raw scores (logits) BEFORE softmax\n",
    "# For multi:softmax classifier, use the booster to get margins:\n",
    "raw_scores = clf.get_booster().inplace_predict(X_sample, strict_shape=True, iteration_range=(0, 0), predict_type=\"margin\")\n",
    "# Note: depending on xgboost version, you may need:\n",
    "# raw_scores = clf.get_booster().inplace_predict(X_sample, predict_type=\"margin\")\n",
    "\n",
    "# raw_scores shape: (n_samples, n_classes)\n",
    "print(\"Raw scores (logits):\\n\", raw_scores)\n",
    "\n",
    "# Softmax to get probabilities\n",
    "def softmax(z):\n",
    "    z = z - np.max(z, axis=1, keepdims=True)  # for numerical stability\n",
    "    expz = np.exp(z)\n",
    "    return expz / expz.sum(axis=1, keepdims=True)\n",
    "\n",
    "probs = softmax(raw_scores)\n",
    "print(\"\\nProbabilities via softmax:\\n\", probs)\n",
    "\n",
    "# Compare to model's predict (class indices)\n",
    "print(\"\\nPredicted classes:\", clf.predict(X_sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c599bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - np.max(z, axis=1, keepdims=True)\n",
    "    expz = np.exp(z)\n",
    "    return expz / expz.sum(axis=1, keepdims=True)\n",
    "\n",
    "booster = clf.get_booster()\n",
    "\n",
    "# Number of boosting rounds (NOT number of trees)\n",
    "try:\n",
    "    n_rounds = booster.num_boosted_rounds()\n",
    "except AttributeError:\n",
    "    # Fallback: total trees / n_classes\n",
    "    n_classes = len(np.unique(y))\n",
    "    n_rounds = len(booster.get_dump()) // n_classes\n",
    "\n",
    "# Pick a single sample to trace (first test row)\n",
    "s = X_test[0:1]\n",
    "\n",
    "cum = np.zeros((1, len(np.unique(y))), dtype=float)\n",
    "print(\"Round | logits (cum) -> probs\")\n",
    "for t in range(n_rounds):\n",
    "    # Contribution from round t (includes all class trees for that round)\n",
    "    round_margin = booster.inplace_predict(\n",
    "        s,\n",
    "        iteration_range=(t, t+1),  # [t, t+1): one boosting round\n",
    "        predict_type=\"margin\"\n",
    "    )\n",
    "    cum += round_margin  # learning_rate already applied internally\n",
    "    print(\n",
    "        f\"{t+1:5d} | {np.round(cum, 3)} -> {np.round(softmax(cum), 3)}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nModel predict_proba for comparison:\",\n",
    "      np.round(clf.predict_proba(s), 3))\n",
    "print(\"Model predicted class:\", clf.predict(s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7acb1d2",
   "metadata": {},
   "source": [
    "<a id=\"diabetes\"></a>\n",
    "## 1.2 Diabetes — Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "Xd, yd = diabetes.data, diabetes.target\n",
    "\n",
    "Xd_train, Xd_test, yd_train, yd_test = train_test_split(\n",
    "    Xd, yd, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "reg = xgb.XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42\n",
    ")\n",
    "reg.fit(Xd_train, yd_train)\n",
    "pred = reg.predict(Xd_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(yd_test, pred))\n",
    "mae = mean_absolute_error(yd_test, pred)\n",
    "print(f\"RMSE: {rmse:.3f} | MAE: {mae:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06082cd2",
   "metadata": {},
   "source": [
    "<a id=\"cancer\"></a>\n",
    "## 1.3 Breast Cancer — Binary Classification + ROC/AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a1489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "bc = load_breast_cancer()\n",
    "Xb, yb = bc.data, bc.target\n",
    "\n",
    "Xb_train, Xb_test, yb_train, yb_test = train_test_split(\n",
    "    Xb, yb, test_size=0.2, random_state=42, stratify=yb\n",
    ")\n",
    "\n",
    "bin_clf = xgb.XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "bin_clf.fit(Xb_train, yb_train)\n",
    "\n",
    "proba = bin_clf.predict_proba(Xb_test)[:, 1]\n",
    "auc = roc_auc_score(yb_test, proba)\n",
    "print(f\"ROC AUC: {auc:.3f}\")\n",
    "\n",
    "RocCurveDisplay.from_predictions(yb_test, proba)\n",
    "plt.title(\"ROC Curve — Breast Cancer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8002fc7",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"phase2\"></a>\n",
    "# Phase 2 — Pipelines & Early Stopping\n",
    "Move toward realistic preprocessing using `ColumnTransformer` and `Pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a445ea",
   "metadata": {},
   "source": [
    "<a id=\"synthetic\"></a>\n",
    "## 2.1 Synthetic Classification — Numeric + Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e51c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Synthetic numeric\n",
    "X_num, y = make_classification(\n",
    "    n_samples=3000, n_features=6, n_informative=4, class_sep=1.2, random_state=42\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(X_num, columns=[f\"num_{i}\" for i in range(6)])\n",
    "rng = np.random.default_rng(42)\n",
    "df[\"cat_color\"] = rng.choice([\"red\", \"green\", \"blue\"], size=len(df))\n",
    "df[\"cat_size\"] = rng.choice([\"S\", \"M\", \"L\", \"XL\"], size=len(df))\n",
    "\n",
    "num_cols = [c for c in df.columns if c.startswith(\"num_\")]\n",
    "cat_cols = [\"cat_color\", \"cat_size\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", \"passthrough\", num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "])\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipe = Pipeline([(\"pre\", pre), (\"model\", model)])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "proba = pipe.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, proba)\n",
    "print(f\"Pipeline ROC AUC: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26336635",
   "metadata": {},
   "source": [
    "<a id=\"early\"></a>\n",
    "## 2.2 Regression with Early Stopping + Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce39928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "d = load_diabetes()\n",
    "X, y = d.data, d.target\n",
    "\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_tmp, y_tmp, test_size=0.5, random_state=42)\n",
    "\n",
    "reg_es = xgb.XGBRegressor(\n",
    "    n_estimators=4000,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.02,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "reg_es.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "pred = reg_es.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "print(\"Best iteration:\", reg_es.best_iteration)\n",
    "print(f\"RMSE (test): {rmse:.3f}\")\n",
    "\n",
    "xgb.plot_importance(reg_es, height=0.5)\n",
    "plt.title(\"Feature Importance — Diabetes (Early Stopping)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c698786c",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"extras\"></a>\n",
    "## Extras — Minimal GridSearchCV Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45565e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    Xb, yb, test_size=0.2, random_state=42, stratify=yb\n",
    ")\n",
    "\n",
    "base = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [200, 400],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 1.0],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(base, param_grid, scoring=\"roc_auc\", cv=3, n_jobs=-1)\n",
    "gs.fit(Xb_train, yb_train)\n",
    "\n",
    "print(\"Best params:\", gs.best_params_)\n",
    "auc = roc_auc_score(yb_test, gs.best_estimator_.predict_proba(Xb_test)[:, 1])\n",
    "print(f\"Test ROC AUC: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d920fb6",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "- Drop real CSVs under `data/raw/` and adapt the `ColumnTransformer` pipeline.\n",
    "- Consider exporting processed features to `data/processed/`.\n",
    "- If you have a GPU, pass `tree_method=\"gpu_hist\"` in the XGBoost constructors.\n",
    "- Move reusable code into `src/` (e.g., preprocessing utilities, plotting, SHAP explainers)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
